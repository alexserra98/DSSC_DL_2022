{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning course - LAB 8\n",
    "\n",
    "## Feature Visualization in ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* we learned to build some popular Convolutional Neural Network (CNN) architectyres\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* we will learn about two main staples of Feature Visualization (FV) in CNNs:\n",
    "    * saliency maps\n",
    "    * Deep Dream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization\n",
    "\n",
    "FV is one of the main tools of Explainable AI (XAI) in CNNs. The aim is to answer the following question: «*What are the (visual) features that are being looked at/searched for by the model for a given image or class?*»\n",
    "\n",
    "The field of research is wide and a lot of work has been done on this field, yet, there is still so much to learn and the framework is all but unified. The main reference for the works in this field are contained in the e-journal [distill.pub](distill.pub), and namely the articles by Christopher Olah and his group @ OpenAI [1](https://distill.pub/2017/feature-visualization/), who concentrate on analyzing what single neurons or convolutional filters learn in specific CNN architectures and weights configuration. It's a daunting task because you have to scrutinize single neurons or filters in a huge CNN and analyze response to 1+ million images. The results, though, are staggering, and really let us draw considerations on the capability of the network, especially considering the aspect of **generalization**.\n",
    "\n",
    "Below, a representation of a neuron in a ResNet50 trained on ImageNet, built as an «artificial, optimized image that maximizes activations of the given unit.»\n",
    "\n",
    "![](imgs/08/trump_neurons.jpg)\n",
    "\n",
    "Image from [OpenAI Microscope](https://microscope.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/89).\n",
    "\n",
    "Note: in the same layer, there are 2559 more neurons to analyze...\n",
    "\n",
    "Note2: all the implementation below have been heavily inspired by [2](https://github.com/utkuozbulak/pytorch-cnn-visualizations). The author did an amazing job by gathering an exhaustive amount of techniques for XAI, but unfortunately the code is not polished, the repo is not a library, and there's a stubborn use of NumPy when torch methods are available nonetheless. If you want, you may fork the library and post a pull request of modernized code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency Maps\n",
    "\n",
    "Saliency Maps (SMs) [3] are one of the first example of FV for CNNs. The motivation behind them is to search which pixel of a given image was more *salient* in the classification of an image in a given class $c\\in \\{1,\\dots,C\\}$.\n",
    "\n",
    "To rephrase it, we wish to find an approximation of which part of an image was deemed more important by the CNN relative to the given class $c$.\n",
    "\n",
    "The intuition behind it is simple: pick an image $I_0$, forward-propagate it through the CNN, pick a class $c$, then backpropagate the gradient **on the image itself** (NB: we backpropagate on the pixels, not on the weights).\n",
    "\n",
    "Let us call $M\\in\\mathbb{R}^{h\\times w}$ the SM, and $M_{ij}$ the map for pixel $i, j$.\n",
    "\n",
    "In the case of a grayscale image $I_0\\in\\mathbb{R}^{h\\times w}$, the SM is:\n",
    "\n",
    "$M_{ij} = \\vert \\frac{\\partial \\text{score}[c]}{\\partial I_0}  \\vert$\n",
    "\n",
    "where $\\text{score}$ is the prediction of the model.\n",
    "\n",
    "If we have a multi-channel (e.g. color) image $I_0\\in\\mathbb{R}^{h\\times w\\times \\text{ch}}$, than the SM is:\n",
    "\n",
    "$M_{ij} = \\max_{\\text{ch}}\\{\\vert \\frac {\\partial \\text{score}[c]} {\\partial I_0}  \\vert\\}$\n",
    "\n",
    "i.e., for each pixel we select the maximum value between the channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "The implementation is simple. We just need to:\n",
    "* set the model in `eval` mode, to avoid recalculation of batchnorm and application of dropout\n",
    "* forward propagate the image, remembering to set `requires_grad` to `True`\n",
    "* backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install get_image_size\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import timm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saliency_map(sample, net, category=None):\n",
    "    '''\n",
    "    if category is None, saliency is calculated on the predicted category\n",
    "    '''\n",
    "    assert len(sample.shape) == 4, f\"Expected input tensor of shape (1 x ch x h x w). Found {list(sample.shape)}\"\n",
    "    # 1. set to eval and erase previous gradient\n",
    "    net.eval()\n",
    "    net.zero_grad()\n",
    "    # 2. switch requires_grad for image\n",
    "    sample.requires_grad = True\n",
    "    # 3. forward propagate image\n",
    "    y_hat = net(sample)\n",
    "\n",
    "    # 4. select category -- if None it is the argmax\n",
    "    if category is None:\n",
    "        category = y_hat.argmax().item()\n",
    "    category_onehot = torch.zeros(y_hat.shape)\n",
    "    category_onehot[:, category] = 1\n",
    "    \n",
    "    # 5. get score and backprop\n",
    "    y_hat.backward(gradient=category_onehot)\n",
    "\n",
    "    saliency_map = sample.grad[0].abs()\n",
    "    # special case -- if image is RGB, I need to do channel-wise max\n",
    "    if len(sample.shape) == 4 and sample.shape[1] > 1:\n",
    "        saliency_map = saliency_map.max(dim=0)[0]\n",
    "    \n",
    "    # return map normalized in 0-1\n",
    "    saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min()) \n",
    "    return saliency_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply it on a ResNet152D model pre-trained on ImageNet. We will use TIMM API for this pretrained model, since there's not much clarity about the preprocessing phase of images for torchvision pretrained models:\n",
    "![]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = timm.create_model(\"resnet152d\", pretrained=True)\n",
    "config = timm.data.resolve_data_config({}, model=net)\n",
    "transform = timm.data.transforms_factory.create_transform(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from this transform, we implement a helper function to *undo the last two transformations*, i.e. recover an image from the 3d or 4d tensor which PyTorch requires as input to the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2img(transform, img):\n",
    "    if len(img.shape) == 4:\n",
    "        out = img[0].permute(1,2,0)\n",
    "    else:\n",
    "        out = img.permute(1,2,0)\n",
    "    out.requires_grad = False\n",
    "    out *= transform.transforms[-1].std\n",
    "    out += transform.transforms[-1].mean\n",
    "    out[out>1] = 1\n",
    "    out[out<0] = 0\n",
    "    out *= 255\n",
    "    out = out.numpy().astype(\"uint8\")\n",
    "    return Image.fromarray(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load an image from the validation set of ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_raw = Image.open(\"imgs/08/ILSVRC2012_val_00000028.JPEG\")\n",
    "img = transform(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showcase of tensor2img use\n",
    "img_cropped = tensor2img(transform, img)\n",
    "img_cropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an empty dimension to the tensor to be able to feed the image to the CNN. A PyTorch CNN expects the input to be of shape $n\\times \\text{ch} \\times h \\times w$, where $n$ is the number of units in the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.unsqueeze(0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the output class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(img).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 166 corresponds to \"Walker hound\", while the ground truth is 167 → \"English foxhound\" (small misclassification error committed by the network)\n",
    "\n",
    "Let us derive the saliency map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smap = get_saliency_map(img, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(smap, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we may loosely visualize the contours of the hound's face within the map...\n",
    "\n",
    "We can use `cv2` `addWeighted` function to overlap the two images and have a better feeling about what the saliency map is looking at within the image. We could do it better by further processing the image, but this viz will be fine for our lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_saliency_and_img(saliency_map, image):\n",
    "    h, w = smap.shape\n",
    "    smap_img = cv2.cvtColor((saliency_map * 255).numpy().astype(\"uint8\"), cv2.COLOR_GRAY2RGB)\n",
    "    img_np = np.array(image)\n",
    "    plt.imshow(cv2.addWeighted(img_np, .15, smap_img, .85, 0))\n",
    "\n",
    "merge_saliency_and_img(smap, img_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with a different image, this one representing a *snowmobile*. This time, the model classifies it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_raw = Image.open(\"imgs/08/ILSVRC2012_val_00000131.JPEG\")\n",
    "img = transform(img_raw)\n",
    "img = img.unsqueeze(0)\n",
    "img_cropped = tensor2img(transform, img)\n",
    "\n",
    "smap = get_saliency_map(img, net.cpu())\n",
    "img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(smap, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the saliency map is actually pretty focussed on the two snowmobiles: moreover, it looks at the man standing in the picture, plus a small part of the background, though, the maximum concentration seems to be on the two cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_saliency_and_img(smap, img_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepDream\n",
    "\n",
    "DeepDream ([4]) is a technique for \"visualizing by optimization\". Namely, we wish to visualize a channel $\\kappa\\in\\{1,\\dots,\\text{ch}\\}$ from the output a given convolutional layer $l$, and we do it by *tweaking* an input image $I\\in\\mathbb{R}^{h\\times w}$ to maximally activate that channel.\n",
    "\n",
    "The core concept behind it is easy: we take an image, possibly (but not necessarily) from the train/test set and we forward it through the net until we have reached the layer $l$.\n",
    "\n",
    "The output of $l$ is $a_l \\in \\mathbb{R}^{\\text{ch} \\times h_l\\times w_l}$. We wish to maximally activate the $\\kappa$-th channel of it. We hence do backpropagation from $a_l^{(\\kappa)} \\in \\mathbb{R}^{h_l\\times w_l}$ towards $I$ and we fix the \"weights\", which in this case are the pixels of $I$.\n",
    "\n",
    "The aim of this is to treat the **activation $a_l$ as an objective function** to maximize (*which is equivalent to minimizing its opposite*). The **loss function** is hence the **negative of the mean activation $a_l^{(\\kappa)}$**.\n",
    "\n",
    "Before introducing the implementation, though, we need to introduce the concept of **forward hook** in PyTorch, which will help us in obtaining an **intermediate activation** without tweaking the existing `forward` method or introducing another ad-hoc routine.\n",
    "\n",
    "Note that DeepDream does not give us a channel visualization *in absolute terms*, as the activation we maximize is **dependent upon the input image**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's have a look at the scheme of the network. The network is very deep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite seemingly hard to grasp, the concept behind the forward hooks is simple: we need to create a method with the following signature:\n",
    "\n",
    "```python\n",
    "hook_function(nn_module, input_to_module, output_of_module)\n",
    "```\n",
    "\n",
    "Inside this `hook_function` we will be doing whatever we need to do to store the activation function. Basically, we don't need the input, but just the output. Since the activation is not present as a keyword argument, we will be storing it in an external list (acting then as a \"global\" var)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "def hook_function(module, input_, output):\n",
    "    activations.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function defined as above will be enough to store the activation of any given module.\n",
    "\n",
    "We now need to tell a given layer to **register a hook** during the forward call.\n",
    "\n",
    "We can do it by calling `module.register_forward_hook(hook_function)`.\n",
    "\n",
    "Supposing we wish to store the activation of the first ReLU within the first Bottleneck layer, we must first track the name of this module within our net, then do the registration.\n",
    "\n",
    "*Tip: to find the names of the layers within a given network, use `net.modules()` and `net.named_modules()` to find the correct name of a module. Note also that `nn.Sequential` stores its modules as a list (so you can access them with `__getitem__`), while normally you access the module as `net.module_name`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the given module\n",
    "net.layer1[0].conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the output of `register_forward_hook` in a variable, so later we will be able to call `handle.remove()` to remove the hook in case we don't want to use it anymore.\n",
    "\n",
    "Let us test if the hook works. We will call `forward` on the net..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = net.layer1[0].conv1.register_forward_hook(hook_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = net(img)\n",
    "print(\"Does the image require grad?\", img.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and see what's now inside `activations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the output is a 64-channel \"image\".\n",
    "\n",
    "We can now remove the handle to start our implementation of DeepDream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute an optimization **in the input space**, we need to create an optimizer which **acts only on the input**. We don't want to accidentally update the parameters of our model by calling `optimizer.step()`.\n",
    "\n",
    "Ideally, it'll be like this:\n",
    "```py\n",
    "optimizer = torch.optim.SGD([image_to_optimize], lr=12, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "Moreover, we'll ask the user which **module** and **filter** of our net will be visualized.\n",
    "\n",
    "Eventually, we'll store intermediate representations (*dreams*) of our session to see how the *dream* forms as more and more iteration of the routine are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deep_dream(image:torch.Tensor, num_ite, net, module, num_filter, optim_fn=lambda img: torch.optim.SGD([img], lr=12, weight_decay=1e-4), ite_interm_store=50, device=None):\n",
    "    assert len(image.shape) == 4, f\"4-d tensor required as image. Found {image.shape}\"\n",
    "    # set up device and send data and model to device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    net.to(device)\n",
    "\n",
    "    image = image.to(device)\n",
    "    # switch requires_grad on so we can backprop gradient to it\n",
    "    image = torch.autograd.Variable(image, requires_grad=True)\n",
    "\n",
    "    # set up hook function and register it to given module\n",
    "    activations = []\n",
    "    def hook_function(module, input_, output):\n",
    "        activations.append(output)\n",
    "    handle = module.register_forward_hook(hook_function)\n",
    "\n",
    "    # instantiate optimizer with image as params group\n",
    "    optimizer = optim_fn(image)\n",
    "\n",
    "    # this list will hold the intermediate and final result\n",
    "    dreams = []\n",
    "    for i in range(num_ite):\n",
    "        # 1. setup: zero gradient and deplete previous activations\n",
    "        optimizer.zero_grad()\n",
    "        activations = []\n",
    "\n",
    "        # 2. do a forward pass to activate the hook(s)\n",
    "        _ = net(image)\n",
    "\n",
    "        # 3. retrieve the activation from the newly populated list\n",
    "        activation = activations[-1][0, num_filter]\n",
    "\n",
    "        # 4. we want to modify the input s.t. it maximally activate the layer -> minimize the negative activation\n",
    "        loss = -torch.mean(activation)\n",
    "\n",
    "        # 5. backpropagate and step to update the input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6. store intermediate result and print loss\n",
    "        if (i+1) % ite_interm_store == 0 or (i+1) == num_ite:\n",
    "            print(f\"Ite {i+1}: loss {loss.item()}\")\n",
    "            dreams.append(image.cpu().clone())\n",
    "    \n",
    "    # remove the hook\n",
    "    handle.remove()\n",
    "    # clean up GPU memo if used\n",
    "    net.cpu()\n",
    "    image.cpu()\n",
    "    return dreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try our implementation on our own pretrained `net`. Remember: layers close to the input are expected to recognize low-level features like texture, color, orientation... on the other hand, the closer to the output, the more complicated the feature.\n",
    "\n",
    "Let's try with something simple..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams = deep_dream(transform(img_raw).unsqueeze(0), num_ite=250, net=net, module=net.layer1[0].conv1, num_filter=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the previously defined `tensor2img` to plot the last iteration of the *dreams*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2img(transform, dreams[4].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided the CNN supports this (e.g. no tie to the image shape) we may also experiment by passing images with a size different than (256, 256) and not from ImageNet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Image.open(\"img/catwide.jpg\")\n",
    "cat = cat.resize((int(1034/2.5), int(652/2.5)), Image.ANTIALIAS) # we reduce a bit the size of the img because we don't want to clog our GPU...\n",
    "cat_tensor = T.Compose(transform.transforms[2:])(cat).unsqueeze(0)\n",
    "cat_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try it on a more advanced layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams_cat = deep_dream(cat_tensor.clone().detach(), num_ite=250, net=net, module=net.layer3[1].act3, num_filter=333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2img(transform, dreams_cat[-1].clone().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often, it will be very hard to interpret the *dream*, because the combination of features may be not straightforward as we expected. What is in the image? Are those cat's ears at various positions and with some degree of variation in the orientations? Are those waves?\n",
    "\n",
    "Let's try with another one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams_cat = deep_dream(cat_tensor.clone().detach(), num_ite=250, net=net, module=net.layer3[22].act3, num_filter=74)\n",
    "tensor2img(transform, dreams_cat[-1].clone().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1](https://distill.pub/2017/feature-visualization/) warns about the fact that some of these visualization may be misleading:\n",
    "\n",
    "> If you want to visualize features, you might just optimize an image to make neurons fire. Unfortunately, this doesn’t really work. Instead, you end up with a kind of neural network optical illusion — an image full of noise and nonsensical high-frequency patterns that the network responds strongly to. \\[...\\]\n",
    "\n",
    "> If you optimize long enough, you’ll tend to see some of what the neuron genuinely detects as well, but the image is dominated by these high frequency patterns. These patterns seem to be closely related to the phenomenon of adversarial examples.\n",
    "\n",
    "and\n",
    "\n",
    "> We don’t fully understand why these high frequency patterns form, but an important part seems to be strided convolutions and pooling operations, which create high-frequency patterns in the gradient \n",
    "\n",
    "> These are certainly interesting, but if we want to understand how these models work in real life, we need to somehow move past them… \n",
    "\n",
    "#### Consideration on the model\n",
    "\n",
    "Let us also consider that ResNets are not as straightforward as other \"sequential\" models, like VGG.\n",
    "\n",
    "ResNets learn residuals, hence the feature learned may not be as interpretable as those in VGG. Let's try with a pretrained VGG19.\n",
    "\n",
    "We will use the one from `torchvision`. We actually select only the convolutional part (`features`), discarding the rest since we're not interested in doing predictions with our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = torchvision.models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms as of pytorch site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_vgg = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(torch.Tensor([0.485, 0.456, 0.406]), torch.Tensor([0.229, 0.224, 0.225]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a copy of our previous image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgg = transform_vgg(img_raw).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheme of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do some tests with increasing level of complexity, as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams_vgg = deep_dream(imgg.clone(), 250, vgg19, vgg19[8], num_filter=2, ite_interm_store=50)\n",
    "tensor2img(transform_vgg, dreams_vgg[-1].detach().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams_vgg = deep_dream(imgg.clone(), 250, vgg19, vgg19[17], num_filter=0, ite_interm_store=50)\n",
    "tensor2img(transform_vgg, dreams_vgg[4].detach().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreams_vgg = deep_dream(imgg.clone(), 60, vgg19, vgg19[23], num_filter=60, ite_interm_store=10)\n",
    "tensor2img(transform_vgg, dreams_vgg[-1].detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and third example are very interesting. It'd seem that they are maximally activated by oriented eyes (2) and pine needles or wolf-pelt-like textures oriented in different ways (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla filter viz\n",
    "\n",
    "Note: if instead of a regular image we pass a randomly generated image of ~ gray colors, we obtain the **filter visualization** technique instead of DeepDream.\n",
    "\n",
    "Let us create a random 224x224 RGB image with colors in the interval 150-180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minval = 150/255 # we normalize in 0-1 because we directly create the tensor instead of passing through the image\n",
    "maxval = 180/255\n",
    "randimg = torch.rand((3, 224, 224)) * (maxval - minval) + minval # constrain the output in the 150/255 - 180/255 interval\n",
    "randimg = transform_vgg.transforms[-1](randimg).unsqueeze(0) # apply the T.Normalize transformation only\n",
    "randimg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this case, it's suggested to use Adam with a high learning rate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = lambda img: torch.optim.Adam([img], lr=.1, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_viz = deep_dream(randimg.clone(), 100, vgg19, vgg19[17], num_filter=0, ite_interm_store=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2img(transform_vgg, layer_viz[-1].detach().clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see in this case that the channel viz is actually pretty similar w.r.t. to the \"deep-dreamt\" one, which suggest that the given filter is **highly specialized towards recognizing eye-like objects**.\n",
    "\n",
    "See also this other example\n",
    "\n",
    "![](img/ddream_l17_f0_iter210.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "There are a lot of FV techniques in the literature. You can categorize them in two broad classes:\n",
    "\n",
    "1. backprop- and gradient-based techniques, like Saliency Maps\n",
    "2. optimization-based techniques, like DeepDream and Channel Visualization\n",
    "\n",
    "If you're interested in doing a review of this field, I would suggest to start from [2](https://github.com/utkuozbulak/pytorch-cnn-visualizations), read the various papers cited there, then move on to [1](https://distill.pub/2017/feature-visualization/) and the following work by OpenAI for the second category of techniques. If you're interesed in applying these techniques in PyTorch, there seem to be a [PT porting of Lucid](https://github.com/greentfrapp/lucent) (which is the original Tensorflow lib developed by Olah's team whose results you may see in their [distill.pub](distill.pub) papers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1](https://distill.pub/2017/feature-visualization/) Olah, Cristopher and al. (2017) \"Feature Visualization\" @ distill.pub → this is the (prologue of the) bible of XAI for what concerns Convolutional Neural Networks. Olah and his team did and are still doing a huge body of work in researching what neurons do and mean in CNNs and what types of neurons there are. Check out their work on [distill.pub](distill.pub).\n",
    "\n",
    "[2](https://github.com/utkuozbulak/pytorch-cnn-visualizations) PyTorch CNN Visualizations, an extensive collection of XAI techniques for CNNs. Unfortunately, the repo is not a library and would need some amount of modernization in the code, as the application is limited only to `torch.nn.Sequential` modules and there's a stubborn use of NumPy which renders the majority of the techniques unusable with a GPU.\n",
    "\n",
    "[3](https://arxiv.org/abs/1312.6034) K. Simonyan, A. Vedaldi, A. Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n",
    "\n",
    "[4](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) A. Mordvintsev, C. Olah, M. Tyka. Inceptionism: Going Deeper into Neural Networks \n",
    "\n",
    "[5](https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network) D. Erhan, Y. Bengio, A. Courville, P. Vincent. Visualizing Higher-Layer Features of a Deep Network "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "416c8164443044ff02f1c27fb42ff95d72f14ca53cfb3789a12d1209117bccde"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "2e6347a50883dfa0598d3f478411c8d6a5b9cf8792810af1a6fbd779ad8b1967"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
